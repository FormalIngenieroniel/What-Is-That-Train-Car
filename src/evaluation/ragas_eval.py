# evaluation/ragas_eval.py
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevance, context_recall
import os
import sys

# A帽adir el directorio ra铆z al path para importar config
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from components.retriever import search_chroma
from components.generator import generate_response

# --- Generaci贸n de Datos de Prueba (Simulaci贸n) ---
# NOTA: Para un proyecto real, generar铆as estas preguntas/respuestas autom谩ticamente
# con un LLM o las escribir铆as manualmente basadas en tus 13 im谩genes.

def run_evaluation():
    
    # Define tus preguntas de prueba
    test_questions = [
        "驴Qu茅 tipo de vag贸n es el de color rojo oscuro y negro dise帽ado para transportar l铆quidos?",
        "驴Cu谩l es la funci贸n del vag贸n verde que tiene una banda blanca horizontal?",
        "Describe el vag贸n de carga de color gris oscuro con el logo rojo de 肖."
    ]
    
    # Crea las respuestas ideales (ground truth)
    ground_truths = [
        "Es un vag贸n cisterna usado para transportar NEFT (petr贸leo), identificado con el logo de 校携 携.",
        "Es un vag贸n cisterna o tolva cubierto para transportar ZERNO (grano), parte del grupo 小校小孝.",
        "Es un vag贸n g贸ndola o caja abierta, acanalado, usado para carga general."
    ]

    # --- Pipeline RAG para RAGAS ---
    data = {
        'question': [],
        'answer': [],
        'contexts': [], # Contextos recuperados por tu sistema
        'ground_truths': ground_truths
    }

    for q in test_questions:
        # 1. Recuperaci贸n
        retrieved_context = search_chroma(q, n_results=3)
        contexts = [c['description'] for c in retrieved_context]
        
        # 2. Generaci贸n
        generated_answer = generate_response(q, retrieved_context)
        
        # Almacenamiento
        data['question'].append(q)
        data['answer'].append(generated_answer)
        data['contexts'].append(contexts)

    # 3. Crear el Dataset de RAGAS
    dataset = Dataset.from_dict(data)

    # 4. Definir las m茅tricas y evaluar
    result = evaluate(
        dataset, 
        metrics=[faithfulness, answer_relevance, context_recall],
        # Especificar el modelo para la evaluaci贸n de RAGAS
        llm=f"gemini/{config.GEMINI_MODEL}" 
    )

    # 5. Imprimir resultados
    print("\n==================  Resultados RAGAS ==================")
    print(result)
    print("\nResultados en formato DataFrame:")
    print(result.to_pandas())
    print("=========================================================")

if __name__ == "__main__":
    run_evaluation()